{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyN5DAbj5emwDj13/wO7st20",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/u-khan902/NaturalLanguageProcessing/blob/main/NLPL2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e95SscyvrBu4",
        "outputId": "a35bdbd9-5a3a-4857-920e-3c0e5fa93d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "nltk.download('twitter_samples')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx5guS9NrngR",
        "outputId": "02d4d5fe-92f6-449e-e317-4cfb6520e4e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "p33CJzl0r5oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdSkmALiv0oT",
        "outputId": "a2a9d3b3-0a53-46c0-dd52-4eb9915ecb30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweet(tweet):\n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  stopwords_english = stopwords.words('english')\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "  tweet = re.sub(r'\\$\\w*','',tweet)\n",
        "  tweet_token  = tokenizer.tokenize(tweet)\n",
        "  for word in tweet_token:\n",
        "    if word not in stopwords_english and word not in string.punctuation:\n",
        "      stemmed_word = stemmer.stem(word)\n",
        "      tweet_token.append(stemmed_word)\n",
        "  return tweet_token"
      ],
      "metadata": {
        "id": "iTi21dGgwBc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_freq(tweets, ys):\n",
        "  ylist = np.squeeze(ys).tolist()\n",
        "  freq = {}\n",
        "  for y, tweet in zip(ylist, tweets):\n",
        "    for word in process_tweet(tweet):\n",
        "      pair = (word, y)\n",
        "      if pair in freq:\n",
        "        freq[pair] += 1\n",
        "      else:\n",
        "        freq[pair] = 1\n",
        "  return freq\n",
        "#\n"
      ],
      "metadata": {
        "id": "p4VNR7wAzW0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select the lists of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# concatenate the lists, 1st part is the positive tweets followed by the negative\n",
        "tweets = all_positive_tweets + all_negative_tweets\n",
        "\n",
        "# let's see how many tweets we have\n",
        "print(\"Number of tweets: \", len(tweets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxoRX4pO2-pG",
        "outputId": "3b019828-e407-4cea-98ac-96b87340c220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tweets:  10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a numpy array representing labels of the tweets\n",
        "labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))"
      ],
      "metadata": {
        "id": "pVNLrFMY3Dn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Soug1C7-3TMl",
        "outputId": "3cd55121-79bf-4aea-9591-829c0bcea274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freq(tweets, labels)\n",
        "\n",
        "# check data type\n",
        "print(f'type(freqs) = {type(freqs)}')\n",
        "\n",
        "# check length of the dictionary\n",
        "print(f'len(freqs) = {len(freqs)}')"
      ],
      "metadata": {
        "id": "OaYkPrp73VPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(freqs)"
      ],
      "metadata": {
        "id": "TFKx67gl3vyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\n",
        "keys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n",
        "        '‚ù§', ':)', ':(', 'üòí', 'üò¨', 'üòÑ', 'üòç', '‚ôõ',\n",
        "        'song', 'idea', 'power', 'play', 'magnific']\n",
        "\n",
        "# list representing our table of word counts.\n",
        "# each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>]\n",
        "data = []\n",
        "\n",
        "# loop through our selected words\n",
        "for word in keys:\n",
        "\n",
        "    # initialize positive and negative counts\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "\n",
        "    # retrieve number of positive counts\n",
        "    if (word, 1) in freqs:\n",
        "        pos = freqs[(word, 1)]\n",
        "\n",
        "    # retrieve number of negative counts\n",
        "    if (word, 0) in freqs:\n",
        "        neg = freqs[(word, 0)]\n",
        "\n",
        "    # append the word counts to the table\n",
        "    data.append([word, pos, neg])\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "Zn4EALUOOFr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize = (8, 8))\n",
        "\n",
        "# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\n",
        "x = np.log([x[1] + 1 for x in data])\n",
        "\n",
        "# do the same for the negative counts\n",
        "y = np.log([x[2] + 1 for x in data])\n",
        "\n",
        "# Plot a dot for each pair of words\n",
        "ax.scatter(x, y)\n",
        "\n",
        "# assign axis labels\n",
        "plt.xlabel(\"Log Positive count\")\n",
        "plt.ylabel(\"Log Negative count\")\n",
        "\n",
        "# Add the word as the label at the same position as you added the points just before\n",
        "for i in range(0, len(data)):\n",
        "    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n",
        "\n",
        "ax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h5x4u72BOIw7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}